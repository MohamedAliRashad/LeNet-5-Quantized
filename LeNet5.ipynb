{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnCdlmZpQhtk",
        "colab_type": "text"
      },
      "source": [
        "# LeNet-5-Quantized\n",
        "\n",
        "In this notebook, i want to demonstrate how i built LeNet-5 in PyTorch and Quantize it for visualization.\n",
        "\n",
        "\n",
        "> Network details in this [Blog post ](https://engmrk.com/lenet-5-a-classic-cnn-architecture/) and architecture view can be found below.\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/2625/1*1TI1aGBZ4dybR6__DI9dzA.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqB-nwesTmVa",
        "colab_type": "text"
      },
      "source": [
        "# Requirments\n",
        "\n",
        "\n",
        "*   PyTorch (torch)\n",
        "*   torchvision\n",
        "*   numpy\n",
        "*   pillow (PIL)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHBVTH3UQaIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import datasets\n",
        "from PIL import Image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doQEQDdYULbp",
        "colab_type": "text"
      },
      "source": [
        "# MNIST Dataset\n",
        "\n",
        "We will download the dataset with `torchvision` and add to it some transforms like padding so the size of the input images become 32x32.\n",
        "\n",
        "We then split the data to `train_data` and `test_data` randomly.\n",
        "\n",
        "We make another split in the training data for validation purposes using `SubsetRandomSampler`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIFlfBRBQLs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20\n",
        "#  validation sample\n",
        "valid_sample = 0.2\n",
        "\n",
        "transform = [transforms.Pad(2), transforms.ToTensor()]\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = datasets.MNIST(root='data',\n",
        "                            train=True,\n",
        "                            download=True,\n",
        "                            transform=transforms.Compose(transform))\n",
        "test_data = datasets.MNIST(root='data',\n",
        "                           train=False,\n",
        "                           download=True,\n",
        "                           transform=transforms.Compose(transform))\n",
        "\n",
        "# Creating validation sampler\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(valid_sample * num_train)\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define sampler for batches\n",
        "trainSampler = SubsetRandomSampler(train_idx)\n",
        "validationSampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = DataLoader(train_data,\n",
        "                          batch_size=batch_size,\n",
        "                          sampler=trainSampler,\n",
        "                          num_workers=num_workers)\n",
        "validation_loader = DataLoader(train_data,\n",
        "                               batch_size=batch_size,\n",
        "                               sampler=validationSampler,\n",
        "                               num_workers=num_workers)\n",
        "test_loader = DataLoader(test_data,\n",
        "                         batch_size=batch_size,\n",
        "                         num_workers=num_workers)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsWvjUVPhFiC",
        "colab_type": "text"
      },
      "source": [
        "# LeNet-5 Network\n",
        "\n",
        "We inherited form `nn.Module` to construct the LeNet-5 architecture in two steps\n",
        "\n",
        "\n",
        "1.   initialization of layers in `__init__`\n",
        "2.   connecting layers to build the pipeline of the network in `forward`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McwMlya_grjs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d393eec7-87d6-4958-d3de-08ce60d00420"
      },
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        # 32 x 32 x 1\n",
        "        self.conv1 = nn.Conv2d(1, 6, (5, 5), padding=0, stride=1)\n",
        "        # 28 x 28 x 6\n",
        "        self.pool1 = nn.AvgPool2d((2, 2), stride=2)\n",
        "        # 14 x 14 x 6\n",
        "        self.conv2 = nn.Conv2d(6, 16, (5, 5), padding=0, stride=1)\n",
        "        # 10 x 10 x 16\n",
        "        self.pool2 = nn.AvgPool2d((2, 2), stride=2)\n",
        "        # 5 x 5 x 16\n",
        "        self.conv3 = nn.Conv2d(16, 120, (5, 5), padding=0, stride=1)\n",
        "        # 1 x 1 x 120\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.fc2 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.tanh(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = torch.tanh(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = torch.tanh(self.conv3(x))\n",
        "        # Choose either view or flatten (as you like)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # x = torch.flatten(x, start_dim=1)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.softmax(self.fc2(x), dim=-1)\n",
        "        return x\n",
        "\n",
        "model = LeNet()\n",
        "print(model)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool1): AvgPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool2): AvgPool2d(kernel_size=(2, 2), stride=2, padding=0)\n",
            "  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrryNEroiEzb",
        "colab_type": "text"
      },
      "source": [
        "# Configurations\n",
        "\n",
        "Here, we configure the loss function to be `CrossEntropyLoss` and the optimizer to be Stochastic Gradient Descent (`SGD`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpqlCK-7iD3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Number of epochs\n",
        "n_epochs=40\n",
        "\n",
        "# classes of MNIST\n",
        "classes = list(range(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXe1FAJliFd8",
        "colab_type": "text"
      },
      "source": [
        "# Training & Validation\n",
        "\n",
        "We first check if a GPU is available so i can transfer the learning to it then,  we put the model in training mode and after every epoch we put the model to `eval` mode so we check the validataion loss is getting better or not to save it in `model.pt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRpO8GkahvH2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1360
        },
        "outputId": "67cb7986-b799-4861-ffa7-c941b7af1d0b"
      },
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    model.train()  # prep model for training\n",
        "    for data, target in train_loader:\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data.to(device))\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target.to(device))\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update running training loss\n",
        "        train_loss += loss.item() * data.size(0)\n",
        "\n",
        "    model.eval()\n",
        "    for data, target in validation_loader:\n",
        "        output = model(data.to(device))\n",
        "        loss = criterion(output, target.to(device))\n",
        "        valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "    # print training statistics\n",
        "    # calculate average loss over an epoch\n",
        "    train_loss = train_loss / len(train_loader.sampler)\n",
        "    valid_loss = valid_loss / len(validation_loader.sampler)\n",
        "\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.\n",
        "          format(epoch + 1, train_loss, valid_loss))\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print(\n",
        "            'Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
        "            .format(valid_loss_min, valid_loss))\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "        valid_loss_min = valid_loss\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 2.300821 \tValidation Loss: 2.298549\n",
            "Validation loss decreased (inf --> 2.298549).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 2.279972 \tValidation Loss: 2.193799\n",
            "Validation loss decreased (2.298549 --> 2.193799).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 1.987013 \tValidation Loss: 1.822721\n",
            "Validation loss decreased (2.193799 --> 1.822721).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 1.745599 \tValidation Loss: 1.692368\n",
            "Validation loss decreased (1.822721 --> 1.692368).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 1.665031 \tValidation Loss: 1.626971\n",
            "Validation loss decreased (1.692368 --> 1.626971).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 1.604691 \tValidation Loss: 1.588060\n",
            "Validation loss decreased (1.626971 --> 1.588060).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 1.579435 \tValidation Loss: 1.569823\n",
            "Validation loss decreased (1.588060 --> 1.569823).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 1.565008 \tValidation Loss: 1.558152\n",
            "Validation loss decreased (1.569823 --> 1.558152).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 1.554564 \tValidation Loss: 1.550071\n",
            "Validation loss decreased (1.558152 --> 1.550071).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 1.546164 \tValidation Loss: 1.539858\n",
            "Validation loss decreased (1.550071 --> 1.539858).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 1.538828 \tValidation Loss: 1.533398\n",
            "Validation loss decreased (1.539858 --> 1.533398).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 1.532506 \tValidation Loss: 1.527512\n",
            "Validation loss decreased (1.533398 --> 1.527512).  Saving model ...\n",
            "Epoch: 13 \tTraining Loss: 1.526829 \tValidation Loss: 1.523227\n",
            "Validation loss decreased (1.527512 --> 1.523227).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 1.522018 \tValidation Loss: 1.517904\n",
            "Validation loss decreased (1.523227 --> 1.517904).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 1.517603 \tValidation Loss: 1.514708\n",
            "Validation loss decreased (1.517904 --> 1.514708).  Saving model ...\n",
            "Epoch: 16 \tTraining Loss: 1.513573 \tValidation Loss: 1.510489\n",
            "Validation loss decreased (1.514708 --> 1.510489).  Saving model ...\n",
            "Epoch: 17 \tTraining Loss: 1.510104 \tValidation Loss: 1.507299\n",
            "Validation loss decreased (1.510489 --> 1.507299).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 1.506749 \tValidation Loss: 1.504840\n",
            "Validation loss decreased (1.507299 --> 1.504840).  Saving model ...\n",
            "Epoch: 19 \tTraining Loss: 1.503845 \tValidation Loss: 1.502418\n",
            "Validation loss decreased (1.504840 --> 1.502418).  Saving model ...\n",
            "Epoch: 20 \tTraining Loss: 1.501295 \tValidation Loss: 1.499505\n",
            "Validation loss decreased (1.502418 --> 1.499505).  Saving model ...\n",
            "Epoch: 21 \tTraining Loss: 1.498957 \tValidation Loss: 1.497844\n",
            "Validation loss decreased (1.499505 --> 1.497844).  Saving model ...\n",
            "Epoch: 22 \tTraining Loss: 1.496894 \tValidation Loss: 1.496691\n",
            "Validation loss decreased (1.497844 --> 1.496691).  Saving model ...\n",
            "Epoch: 23 \tTraining Loss: 1.495146 \tValidation Loss: 1.495136\n",
            "Validation loss decreased (1.496691 --> 1.495136).  Saving model ...\n",
            "Epoch: 24 \tTraining Loss: 1.493542 \tValidation Loss: 1.494458\n",
            "Validation loss decreased (1.495136 --> 1.494458).  Saving model ...\n",
            "Epoch: 25 \tTraining Loss: 1.492070 \tValidation Loss: 1.492606\n",
            "Validation loss decreased (1.494458 --> 1.492606).  Saving model ...\n",
            "Epoch: 26 \tTraining Loss: 1.490772 \tValidation Loss: 1.491522\n",
            "Validation loss decreased (1.492606 --> 1.491522).  Saving model ...\n",
            "Epoch: 27 \tTraining Loss: 1.489666 \tValidation Loss: 1.490604\n",
            "Validation loss decreased (1.491522 --> 1.490604).  Saving model ...\n",
            "Epoch: 28 \tTraining Loss: 1.488536 \tValidation Loss: 1.490729\n",
            "Epoch: 29 \tTraining Loss: 1.487668 \tValidation Loss: 1.489263\n",
            "Validation loss decreased (1.490604 --> 1.489263).  Saving model ...\n",
            "Epoch: 30 \tTraining Loss: 1.486728 \tValidation Loss: 1.488038\n",
            "Validation loss decreased (1.489263 --> 1.488038).  Saving model ...\n",
            "Epoch: 31 \tTraining Loss: 1.485958 \tValidation Loss: 1.487488\n",
            "Validation loss decreased (1.488038 --> 1.487488).  Saving model ...\n",
            "Epoch: 32 \tTraining Loss: 1.485237 \tValidation Loss: 1.486905\n",
            "Validation loss decreased (1.487488 --> 1.486905).  Saving model ...\n",
            "Epoch: 33 \tTraining Loss: 1.484495 \tValidation Loss: 1.486666\n",
            "Validation loss decreased (1.486905 --> 1.486666).  Saving model ...\n",
            "Epoch: 34 \tTraining Loss: 1.483838 \tValidation Loss: 1.485787\n",
            "Validation loss decreased (1.486666 --> 1.485787).  Saving model ...\n",
            "Epoch: 35 \tTraining Loss: 1.483274 \tValidation Loss: 1.485637\n",
            "Validation loss decreased (1.485787 --> 1.485637).  Saving model ...\n",
            "Epoch: 36 \tTraining Loss: 1.482690 \tValidation Loss: 1.484685\n",
            "Validation loss decreased (1.485637 --> 1.484685).  Saving model ...\n",
            "Epoch: 37 \tTraining Loss: 1.482182 \tValidation Loss: 1.484329\n",
            "Validation loss decreased (1.484685 --> 1.484329).  Saving model ...\n",
            "Epoch: 38 \tTraining Loss: 1.481720 \tValidation Loss: 1.483758\n",
            "Validation loss decreased (1.484329 --> 1.483758).  Saving model ...\n",
            "Epoch: 39 \tTraining Loss: 1.481211 \tValidation Loss: 1.483678\n",
            "Validation loss decreased (1.483758 --> 1.483678).  Saving model ...\n",
            "Epoch: 40 \tTraining Loss: 1.480738 \tValidation Loss: 1.482894\n",
            "Validation loss decreased (1.483678 --> 1.482894).  Saving model ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsYotXAkrqLm",
        "colab_type": "text"
      },
      "source": [
        "# Testing\n",
        "\n",
        "Going through the testing dataset to get the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb0m0-sTrO98",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "0ab19b7c-0659-4dc9-c25a-c93d0233c559"
      },
      "source": [
        "# initialize lists to monitor test loss and accuracy\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval()  # prep model for *evaluation*\n",
        "\n",
        "for data, target in test_loader:\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the loss\n",
        "    loss = criterion(output, target)\n",
        "    # update test loss\n",
        "    test_loss += loss.item() * data.size(0)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)\n",
        "    # compare predictions to true label\n",
        "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(batch_size):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# calculate and print avg test loss\n",
        "test_loss = test_loss / len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' %\n",
        "              (str(i), 100 * class_correct[i] / class_total[i],\n",
        "               np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' %\n",
        "              (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' %\n",
        "      (100. * np.sum(class_correct) / np.sum(class_total),\n",
        "       np.sum(class_correct), np.sum(class_total)))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 1.482806\n",
            "\n",
            "Test Accuracy of     0: 98% (970/980)\n",
            "Test Accuracy of     1: 99% (1124/1135)\n",
            "Test Accuracy of     2: 98% (1016/1032)\n",
            "Test Accuracy of     3: 98% (996/1010)\n",
            "Test Accuracy of     4: 98% (963/982)\n",
            "Test Accuracy of     5: 97% (871/892)\n",
            "Test Accuracy of     6: 98% (940/958)\n",
            "Test Accuracy of     7: 98% (1008/1028)\n",
            "Test Accuracy of     8: 97% (954/974)\n",
            "Test Accuracy of     9: 95% (968/1009)\n",
            "\n",
            "Test Accuracy (Overall): 98% (9810/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85TJK1qYrq1Y",
        "colab_type": "text"
      },
      "source": [
        "# Quantization\n",
        "\n",
        "We deliver a visual representation to the feature maps generated from the learnt weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLTM12heroDx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "76429032-5b78-4370-ec9c-7ad47704710b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "input_img = next(iter(test_loader))[0][0].unsqueeze(0)\n",
        "\n",
        "def quantize_arr(arr):\n",
        "    ''' Quantization based on linear rescaling over min/max range.'''\n",
        "    min_val, max_val = np.min(arr), np.max(arr)\n",
        "\n",
        "    if max_val - min_val > 0:\n",
        "        quantized = np.round(255 * (arr - min_val) / (max_val - min_val))\n",
        "    else:\n",
        "        quantized = np.zeros(arr.shape)\n",
        "\n",
        "    quantized = quantized.astype(np.uint8)\n",
        "    min_val = min_val.astype(np.float32)\n",
        "    max_val = max_val.astype(np.float32)\n",
        "\n",
        "    return quantized, min_val, max_val\n",
        "  \n",
        "plt.figure(figsize=(10, 5))\n",
        "row = 2\n",
        "columns = 3\n",
        "for i in range(6):\n",
        "    output, min_val, max_val = quantize_arr(\n",
        "        model.conv1.forward(input_img)[0][i].detach().numpy())\n",
        "    plt.subplot(3 / columns + 1, columns, i + 1)\n",
        "    plt.imshow(output)\n",
        "    \n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAEyCAYAAAA7n5DmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 720x360 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}